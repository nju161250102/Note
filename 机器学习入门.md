# 《Python机器学习及实践》

### 2.1 监督学习经典模型
准备训练数据——抽取特征向量——和标签一起送入算法训练模型——预测结果  

#### 2.1.1 分类学习
* 线性分类器  

假设特征与结果存在线性关系，通过累加每个维度的特征和各自权重的乘积帮助类别决策。  
线性关系的数学表达：  
<img src="https://latex.codecogs.com/gif.latex?f(\vec{w},\vec{x},b)=\vec{w}^{T}\vec{x}&plus;b"/>  
使用Logistic函数将f映射到(0,1)(对于二分问题而言)：  
<img src="https://latex.codecogs.com/gif.latex?g(x)=\frac{1}{1&plus;e^{-x}}"/>  
随机梯度下降模型计算时间短但是产生的模型性能略低。  
```
# coding=utf-8
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import classification_report

# ——数据加载与清洗——
data = load_breast_cancer()
# data 用于学习的数据 target 分类标签 target_names 数字标签含义 feature_names 特征含义 DESCR 数据集描述
df = pd.DataFrame(data['data'], columns=data['feature_names'])
# 丢弃带缺失值的数据（本数据集实际上没有缺失值）
df = df.replace(to_replace='?', value=np.nan)
df = df.dropna()

# ——数据集分割——
X_train, X_test, y_train, y_test = train_test_split(df, data['target'], test_size=0.25)

# ——数据标准化处理——
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

# 逻辑斯蒂回归模型
lr = LogisticRegression()
lr.fit(X_train, y_train)
lr_y_predict = lr.predict(X_test)
print lr.score(X_test, y_test)
print classification_report(y_test, lr_y_predict, target_names=data['target_names'])

# 随机梯度下降模型
sgdc = SGDClassifier()
sgdc.fit(X_train, y_train)
sgdc_y_predict = sgdc.predict(X_test)
print classification_report(y_test, sgdc_y_predict, target_names=data['target_names'])

```
* 支持向量机

根据训练样本的分布搜索所有可能的线性分类器中最佳的那个。  
节省了模型训练所需的内存，提高了预测性能，但是付出了更多的计算代价。  
```
# coding=utf-8
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

# ——数据加载——
digits = load_digits()
# data 64像素列表表示 image 对应的图片 target 分类标签0-9 target_name 标签名 DESCR 数据集描述

# ——数据集分割——
X_train, X_test, y_train, y_test = train_test_split(digits['data'], digits['target'], test_size=0.25)

# ——数据标准化处理——
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

# ——基于线性假设的支持向量机——
lsvc = LinearSVC()
lsvc.fit(X_train, y_train)
y_predict = lsvc.predict(X_test)
print lsvc.score(X_test, y_test)
print classification_report(y_test, y_predict, target_names=[str(x) for x in digits['target_names']])

```

* 朴素贝叶斯

朴素贝叶斯分类器的构造基础是贝叶斯理论，基本数学假设是，各个维度上的特征被分类的条件概率是相互独立的。因此在数据特征关联性较强的任务上表现不佳。  
```
# coding=utf-8
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# ——数据加载与分割——
news = fetch_20newsgroups(subset="all")
X_train, X_test, y_train, y_test = train_test_split(news['data'], news['target'], test_size=0.25)

# ——数据特征提取——
vec = CountVectorizer()
X_train = vec.fit_transform(X_train)
X_test = vec.transform(X_test)

# ——朴素贝叶斯模型训练——
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_predict = mnb.predict(X_test)

print classification_report(y_test, y_predict, target_names=news['target_names'])

```

* K近邻

属于无参数模型，没有参数训练过程而直接做出分类决策，同时也导致了非常高的计算复杂度和内存消耗（平方级别的算法复杂度）  

```
# coding=utf-8
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# ——数据加载与分割——
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.25)

# ——数据标准化处理——
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

# ——K近邻模型——
knc = KNeighborsClassifier()
knc.fit(X_train, y_train)
y_predict = knc.predict(X_test)

print knc.score(X_test, y_test)
print classification_report(y_test, y_predict, target_names=iris['target_names'])

```
